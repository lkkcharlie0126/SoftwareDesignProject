{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"s2s_aami.ipynb","provenance":[],"authorship_tag":"ABX9TyMgA+Nw0jXTTHxG8iSwjfdO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dqeeiNOou7_","executionInfo":{"status":"ok","timestamp":1640856022464,"user_tz":-480,"elapsed":18671,"user":{"displayName":"溫子謙","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04405529575981230575"}},"outputId":"560a2798-7461-4735-d352-20ef7aeff7a4"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/NCKU/Master/110_1/Software_design/FinalProject/ECG-Heartbeat-Classification-seq2seq-model-master"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1J6BCXfpKqX","executionInfo":{"status":"ok","timestamp":1640856082136,"user_tz":-480,"elapsed":387,"user":{"displayName":"溫子謙","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04405529575981230575"}},"outputId":"af8f2688-dcd1-48bd-b26d-8ff3540da7e6"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NCKU/Master/110_1/Software_design/FinalProject/ECG-Heartbeat-Classification-seq2seq-model-master\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.io as spio\n","from  sklearn.preprocessing import MinMaxScaler\n","import random\n","import time\n","import os\n","from datetime import datetime\n","from sklearn.metrics import confusion_matrix\n","import tensorflow as tf\n","from imblearn.over_sampling import SMOTE\n","from sklearn.model_selection import train_test_split\n","import argparse\n","random.seed(654)"],"metadata":{"id":"YImwUKKqliTn","executionInfo":{"status":"ok","timestamp":1640855339763,"user_tz":-480,"elapsed":350,"user":{"displayName":"溫子謙","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04405529575981230575"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def read_mitbih(filename, max_time=100, classes= ['F', 'N', 'S', 'V', 'Q'], max_nlabel=100):\n","    def normalize(data):\n","        data = np.nan_to_num(data)  # removing NaNs and Infs\n","        data = data - np.mean(data)\n","        data = data / np.std(data)\n","        return data\n","\n","    # read data\n","    data = []\n","    samples = spio.loadmat(filename + \".mat\")\n","    samples = samples['s2s_mitbih']\n","    values = samples[0]['seg_values']\n","    labels = samples[0]['seg_labels']\n","    num_annots = sum([item.shape[0] for item in values])\n","\n","    n_seqs = num_annots / max_time\n","    #  add all segments(beats) together\n","    l_data = 0\n","    for i, item in enumerate(values):\n","        l = item.shape[0]\n","        for itm in item:\n","            if l_data == n_seqs * max_time:\n","                break\n","            data.append(itm[0])\n","            l_data = l_data + 1\n","\n","    #  add all labels together\n","    l_lables  = 0\n","    t_lables = []\n","    for i, item in enumerate(labels):\n","        if len(t_lables)==n_seqs*max_time:\n","            break\n","        item= item[0]\n","        for lebel in item:\n","            if l_lables == n_seqs * max_time:\n","                break\n","            t_lables.append(str(lebel))\n","            l_lables = l_lables + 1\n","\n","    del values\n","    data = np.asarray(data)\n","    shape_v = data.shape\n","    data = np.reshape(data, [shape_v[0], -1])\n","    t_lables = np.array(t_lables)\n","    _data  = np.asarray([],dtype=np.float64).reshape(0,shape_v[1])\n","    _labels = np.asarray([],dtype=np.dtype('|S1')).reshape(0,)\n","    for cl in classes:\n","        _label = np.where(t_lables == cl)\n","        permute = np.random.permutation(len(_label[0]))\n","        _label = _label[0][permute[:max_nlabel]]\n","\n","        # _label = _label[0][:max_nlabel]\n","        # permute = np.random.permutation(len(_label))\n","        # _label = _label[permute]\n","        _data = np.concatenate((_data, data[_label]))\n","        _labels = np.concatenate((_labels, t_lables[_label]))\n","\n","    data = _data[:(len(_data)/ max_time) * max_time, :]\n","    _labels = _labels[:(len(_data) / max_time) * max_time]\n","\n","    # data = _data\n","    #  split data into sublist of 100=se_len values\n","    data = [data[i:i + max_time] for i in range(0, len(data), max_time)]\n","    labels = [_labels[i:i + max_time] for i in range(0, len(_labels), max_time)]\n","    # shuffle\n","    permute = np.random.permutation(len(labels))\n","    data = np.asarray(data)\n","    labels = np.asarray(labels)\n","    data= data[permute]\n","    labels = labels[permute]\n","\n","    print('Records processed!')\n","\n","    return data, labels"],"metadata":{"id":"fsOwihQQlsXf","executionInfo":{"status":"ok","timestamp":1640855341396,"user_tz":-480,"elapsed":3,"user":{"displayName":"溫子謙","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04405529575981230575"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def evaluate_metrics(confusion_matrix):\n","    # https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n","    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n","    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n","    TP = np.diag(confusion_matrix)\n","    TN = confusion_matrix.sum() - (FP + FN + TP)\n","    # Sensitivity, hit rate, recall, or true positive rate\n","    TPR = TP / (TP + FN)\n","    # Specificity or true negative rate\n","    TNR = TN / (TN + FP)\n","    # Precision or positive predictive value\n","    PPV = TP / (TP + FP)\n","    # Negative predictive value\n","    NPV = TN / (TN + FN)\n","    # Fall out or false positive rate\n","    FPR = FP / (FP + TN)\n","    # False negative rate\n","    FNR = FN / (TP + FN)\n","    # False discovery rate\n","    FDR = FP / (TP + FP)\n","\n","    # Overall accuracy\n","    ACC = (TP + TN) / (TP + FP + FN + TN)\n","    # ACC_micro = (sum(TP) + sum(TN)) / (sum(TP) + sum(FP) + sum(FN) + sum(TN))\n","    ACC_macro = np.mean(ACC) # to get a sense of effectiveness of our method on the small classes we computed this average (macro-average)\n","\n","    return ACC_macro, ACC, TPR, TNR, PPV"],"metadata":{"id":"qCkKRtcElyAY","executionInfo":{"status":"ok","timestamp":1640855342931,"user_tz":-480,"elapsed":2,"user":{"displayName":"溫子謙","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04405529575981230575"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def batch_data(x, y, batch_size):\n","    shuffle = np.random.permutation(len(x))\n","    start = 0\n","    #     from IPython.core.debugger import Tracer; Tracer()()\n","    x = x[shuffle]\n","    y = y[shuffle]\n","    while start + batch_size <= len(x):\n","        yield x[start:start + batch_size], y[start:start + batch_size]\n","        start += batch_size"],"metadata":{"id":"qwGGxSrPl1Ok","executionInfo":{"status":"ok","timestamp":1640855344729,"user_tz":-480,"elapsed":535,"user":{"displayName":"溫子謙","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04405529575981230575"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def build_network(inputs, dec_inputs,char2numY,n_channels=10,input_depth=280,num_units=128,max_time=10,bidirectional=False):\n","    _inputs = tf.reshape(inputs, [-1, n_channels, input_depth / n_channels])\n","    # _inputs = tf.reshape(inputs, [-1,input_depth,n_channels])\n","\n","    # #(batch*max_time, 280, 1) --> (N, 280, 18)\n","    conv1 = tf.layers.conv1d(inputs=_inputs, filters=32, kernel_size=2, strides=1,\n","                             padding='same', activation=tf.nn.relu)\n","    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same')\n","\n","    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=2, strides=1,\n","                             padding='same', activation=tf.nn.relu)\n","    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n","\n","    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=128, kernel_size=2, strides=1,\n","                             padding='same', activation=tf.nn.relu)\n","\n","    shape = conv3.get_shape().as_list()\n","    data_input_embed = tf.reshape(conv3, (-1, max_time, shape[1] * shape[2]))\n","\n","    # timesteps = max_time\n","    #\n","    # lstm_in = tf.unstack(data_input_embed, timesteps, 1)\n","    # lstm_size = 128\n","    # # Get lstm cell output\n","    # # Add LSTM layers\n","    # lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n","    # data_input_embed, states = tf.contrib.rnn.static_rnn(lstm_cell, lstm_in, dtype=tf.float32)\n","    # data_input_embed = tf.stack(data_input_embed, 1)\n","\n","    # shape = data_input_embed.get_shape().as_list()\n","\n","    embed_size = 10  # 128 lstm_size # shape[1]*shape[2]\n","\n","    # Embedding layers\n","    output_embedding = tf.Variable(tf.random_uniform((len(char2numY), embed_size), -1.0, 1.0), name='dec_embedding')\n","    data_output_embed = tf.nn.embedding_lookup(output_embedding, dec_inputs)\n","\n","    with tf.variable_scope(\"encoding\") as encoding_scope:\n","        if not bidirectional:\n","\n","            # Regular approach with LSTM units\n","            lstm_enc = tf.contrib.rnn.LSTMCell(num_units)\n","            _, last_state = tf.nn.dynamic_rnn(lstm_enc, inputs=data_input_embed, dtype=tf.float32)\n","\n","        else:\n","\n","            # Using a bidirectional LSTM architecture instead\n","            enc_fw_cell = tf.contrib.rnn.LSTMCell(num_units)\n","            enc_bw_cell = tf.contrib.rnn.LSTMCell(num_units)\n","\n","            ((enc_fw_out, enc_bw_out), (enc_fw_final, enc_bw_final)) = tf.nn.bidirectional_dynamic_rnn(\n","                cell_fw=enc_fw_cell,\n","                cell_bw=enc_bw_cell,\n","                inputs=data_input_embed,\n","                dtype=tf.float32)\n","            enc_fin_c = tf.concat((enc_fw_final.c, enc_bw_final.c), 1)\n","            enc_fin_h = tf.concat((enc_fw_final.h, enc_bw_final.h), 1)\n","            last_state = tf.contrib.rnn.LSTMStateTuple(c=enc_fin_c, h=enc_fin_h)\n","\n","    with tf.variable_scope(\"decoding\") as decoding_scope:\n","        if not bidirectional:\n","            lstm_dec = tf.contrib.rnn.LSTMCell(num_units)\n","        else:\n","            lstm_dec = tf.contrib.rnn.LSTMCell(2 * num_units)\n","\n","        dec_outputs, _ = tf.nn.dynamic_rnn(lstm_dec, inputs=data_output_embed, initial_state=last_state)\n","\n","    logits = tf.layers.dense(dec_outputs, units=len(char2numY), use_bias=True)\n","\n","    return logits"],"metadata":{"id":"nCYbNrtWl54h","executionInfo":{"status":"ok","timestamp":1640855346407,"user_tz":-480,"elapsed":2,"user":{"displayName":"溫子謙","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04405529575981230575"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def str2bool(v):\n","    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n","        return True\n","    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n","        return False\n","    else:\n","        raise argparse.ArgumentTypeError('Boolean value expected.')"],"metadata":{"id":"9Rjqxu--l_mI","executionInfo":{"status":"ok","timestamp":1640855347923,"user_tz":-480,"elapsed":1,"user":{"displayName":"溫子謙","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04405529575981230575"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def main():\n","    parser = argparse.ArgumentParser()\n","    \n","    parser.add_argument('--epochs', type=int, default=500)\n","    parser.add_argument('--max_time', type=int, default=10)\n","    parser.add_argument('--test_steps', type=int, default=10)\n","    parser.add_argument('--batch_size', type=int, default=20)\n","    parser.add_argument('--data_dir', type=str, default='data/s2s_mitbih_aami')\n","    parser.add_argument('--bidirectional', type=str2bool, default=str2bool('False'))\n","    # parser.add_argument('--lstm_layers', type=int, default=2)\n","    parser.add_argument('--num_units', type=int, default=128)\n","    parser.add_argument('--n_oversampling', type=int, default=10000)\n","    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints-seq2seq')\n","    parser.add_argument('--ckpt_name', type=str, default='seq2seq_mitbih.ckpt')\n","    parser.add_argument('--classes', nargs='+', type=chr,\n","                        default=['F','N', 'S','V'])\n","    print(0)\n","    args = parser.parse_args()\n","    print(1)\n","    run_program(args)\n"],"metadata":{"id":"F4N9_mipmCZI","executionInfo":{"status":"ok","timestamp":1640855582130,"user_tz":-480,"elapsed":349,"user":{"displayName":"溫子謙","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04405529575981230575"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Ocavk5oNlAMt","executionInfo":{"status":"ok","timestamp":1640855470928,"user_tz":-480,"elapsed":938,"user":{"displayName":"溫子謙","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04405529575981230575"}}},"outputs":[],"source":["def run_program(args):\n","    print(args)\n","    max_time = args.max_time # 5 3 second best 10# 40 # 100\n","    epochs = args.epochs # 300\n","    batch_size = args.batch_size # 10\n","    num_units = args.num_units\n","    bidirectional = args.bidirectional\n","    # lstm_layers = args.lstm_layers\n","    n_oversampling = args.n_oversampling\n","    checkpoint_dir = args.checkpoint_dir\n","    ckpt_name = args.ckpt_name\n","    test_steps = args.test_steps\n","    classes= args.classes\n","    filename = args.data_dir\n","\n","    X, Y = read_mitbih(filename,max_time,classes=classes,max_nlabel=100000) #11000\n","    print (\"# of sequences: \", len(X))\n","    input_depth = X.shape[2]\n","    n_channels = 10\n","    classes = np.unique(Y)\n","    char2numY = dict(zip(classes, range(len(classes))))\n","    n_classes = len(classes)\n","    print ('Classes: ', classes)\n","    for cl in classes:\n","        ind = np.where(classes == cl)[0][0]\n","        print (cl, len(np.where(Y.flatten()==cl)[0]))\n","    # char2numX['<PAD>'] = len(char2numX)\n","    # num2charX = dict(zip(char2numX.values(), char2numX.keys()))\n","    # max_len = max([len(date) for date in x])\n","    #\n","    # x = [[char2numX['<PAD>']]*(max_len - len(date)) +[char2numX[x_] for x_ in date] for date in x]\n","    # print(''.join([num2charX[x_] for x_ in x[4]]))\n","    # x = np.array(x)\n","\n","    char2numY['<GO>'] = len(char2numY)\n","    num2charY = dict(zip(char2numY.values(), char2numY.keys()))\n","\n","    Y = [[char2numY['<GO>']] + [char2numY[y_] for y_ in date] for date in Y]\n","    Y = np.array(Y)\n","\n","    x_seq_length = len(X[0])\n","    y_seq_length = len(Y[0])- 1\n","\n","    # Placeholders\n","    inputs = tf.placeholder(tf.float32, [None, max_time, input_depth], name = 'inputs')\n","    targets = tf.placeholder(tf.int32, (None, None), 'targets')\n","    dec_inputs = tf.placeholder(tf.int32, (None, None), 'output')\n","\n","    # logits = build_network(inputs,dec_inputs=dec_inputs)\n","    logits = build_network(inputs, dec_inputs, char2numY, n_channels=n_channels, input_depth=input_depth, num_units=num_units, max_time=max_time,\n","                  bidirectional=bidirectional)\n","    # decoder_prediction = tf.argmax(logits, 2)\n","    # confusion = tf.confusion_matrix(labels=tf.argmax(targets, 1), predictions=tf.argmax(logits, 2), num_classes=len(char2numY) - 1)# it is wrong\n","    # mean_accuracy,update_mean_accuracy = tf.metrics.mean_per_class_accuracy(labels=targets, predictions=decoder_prediction, num_classes=len(char2numY) - 1)\n","\n","    with tf.name_scope(\"optimization\"):\n","        # Loss function\n","        vars = tf.trainable_variables()\n","        beta = 0.001\n","        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars\n","                            if 'bias' not in v.name]) * beta\n","        loss = tf.contrib.seq2seq.sequence_loss(logits, targets, tf.ones([batch_size, y_seq_length]))\n","        # Optimizer\n","        loss = tf.reduce_mean(loss + lossL2)\n","        optimizer = tf.train.RMSPropOptimizer(1e-3).minimize(loss)\n","\n","\n","    # split the dataset into the training and test sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","    # over-sampling: SMOTE\n","    X_train = np.reshape(X_train,[X_train.shape[0]*X_train.shape[1],-1])\n","    y_train= y_train[:,1:].flatten()\n","\n","    nums = []\n","    for cl in classes:\n","        ind = np.where(classes == cl)[0][0]\n","        nums.append(len(np.where(y_train.flatten()==ind)[0]))\n","    # ratio={0:nums[3],1:nums[1],2:nums[3],3:nums[3]} # the best with 11000 for N\n","    ratio={0:n_oversampling,1:nums[1],2:n_oversampling,3:n_oversampling}\n","    sm = SMOTE(random_state=12,ratio=ratio)\n","    X_train, y_train = sm.fit_sample(X_train, y_train)\n","\n","    X_train = X_train[:(X_train.shape[0]/max_time)*max_time,:]\n","    y_train = y_train[:(X_train.shape[0]/max_time)*max_time]\n","\n","    X_train = np.reshape(X_train,[-1,X_test.shape[1],X_test.shape[2]])\n","    y_train = np.reshape(y_train,[-1,y_test.shape[1]-1,])\n","    y_train= [[char2numY['<GO>']] + [y_ for y_ in date] for date in y_train]\n","    y_train = np.array(y_train)\n","\n","    print ('Classes in the training set: ', classes)\n","    for cl in classes:\n","        ind = np.where(classes == cl)[0][0]\n","        print (cl, len(np.where(y_train.flatten()==ind)[0]))\n","    print (\"------------------y_train samples--------------------\")\n","    for ii in range(2):\n","      print(''.join([num2charY[y_] for y_ in list(y_train[ii+5])]))\n","    print (\"------------------y_test samples--------------------\")\n","    for ii in range(2):\n","      print(''.join([num2charY[y_] for y_ in list(y_test[ii+5])]))\n","\n","    def test_model():\n","        # source_batch, target_batch = next(batch_data(X_test, y_test, batch_size))\n","        acc_track = []\n","        sum_test_conf = []\n","        for batch_i, (source_batch, target_batch) in enumerate(batch_data(X_test, y_test, batch_size)):\n","\n","            dec_input = np.zeros((len(source_batch), 1)) + char2numY['<GO>']\n","            for i in range(y_seq_length):\n","                batch_logits = sess.run(logits,\n","                                        feed_dict={inputs: source_batch, dec_inputs: dec_input})\n","                prediction = batch_logits[:, -1].argmax(axis=-1)\n","                dec_input = np.hstack([dec_input, prediction[:, None]])\n","            # acc_track.append(np.mean(dec_input == target_batch))\n","            acc_track.append(dec_input[:, 1:] == target_batch[:, 1:])\n","            y_true= target_batch[:, 1:].flatten()\n","            y_pred = dec_input[:, 1:].flatten()\n","            sum_test_conf.append(confusion_matrix(y_true, y_pred,labels=range(len(char2numY)-1)))\n","\n","        sum_test_conf= np.mean(np.array(sum_test_conf, dtype=np.float32), axis=0)\n","\n","        # print('Accuracy on test set is: {:>6.4f}'.format(np.mean(acc_track)))\n","\n","        # mean_p_class, accuracy_classes = sess.run([mean_accuracy, update_mean_accuracy],\n","        #                                           feed_dict={inputs: source_batch,\n","        #                                                      dec_inputs: dec_input[:, :-1],\n","        #                                                      targets: target_batch[:, 1:]})\n","        # print (mean_p_class)\n","        # print (accuracy_classes)\n","        acc_avg, acc, sensitivity, specificity, PPV = evaluate_metrics(sum_test_conf)\n","        print('Average Accuracy is: {:>6.4f} on test set'.format(acc_avg))\n","        for index_ in range(n_classes):\n","            print(\"\\t{} rhythm -> Sensitivity: {:1.4f}, Specificity : {:1.4f}, Precision (PPV) : {:1.4f}, Accuracy : {:1.4f}\".format(classes[index_],\n","                                                                                                          sensitivity[\n","                                                                                                              index_],\n","                                                                                                          specificity[\n","                                                                                                              index_],PPV[index_],\n","                                                                                                          acc[index_]))\n","        print(\"\\t Average -> Sensitivity: {:1.4f}, Specificity : {:1.4f}, Precision (PPV) : {:1.4f}, Accuracy : {:1.4f}\".format(np.mean(sensitivity),np.mean(specificity),np.mean(PPV),np.mean(acc)))\n","        return acc_avg, acc, sensitivity, specificity, PPV\n","    loss_track = []\n","    def count_prameters():\n","        print ('# of Params: ', np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))\n","\n","    count_prameters()\n","\n","    if (os.path.exists(checkpoint_dir) == False):\n","        os.mkdir(checkpoint_dir)\n","    # train the graph\n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","        sess.run(tf.local_variables_initializer())\n","        saver = tf.train.Saver()\n","        print(str(datetime.now()))\n","        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n","        pre_acc_avg = 0.0\n","        if ckpt and ckpt.model_checkpoint_path:\n","            # # Restore\n","            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n","            # saver.restore(session, os.path.join(checkpoint_dir, ckpt_name))\n","            saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n","            # or 'load meta graph' and restore weights\n","            # saver = tf.train.import_meta_graph(ckpt_name+\".meta\")\n","            # saver.restore(session,tf.train.latest_checkpoint(checkpoint_dir))\n","            test_model()\n","        else:\n","\n","            for epoch_i in range(epochs):\n","                start_time = time.time()\n","                train_acc = []\n","                for batch_i, (source_batch, target_batch) in enumerate(batch_data(X_train, y_train, batch_size)):\n","                    _, batch_loss, batch_logits = sess.run([optimizer, loss, logits],\n","                        feed_dict = {inputs: source_batch,\n","                                     dec_inputs: target_batch[:, :-1],\n","                                     targets: target_batch[:, 1:]})\n","                    loss_track.append(batch_loss)\n","                    train_acc.append(batch_logits.argmax(axis=-1) == target_batch[:,1:])\n","                # mean_p_class,accuracy_classes = sess.run([mean_accuracy,update_mean_accuracy],\n","                #                         feed_dict={inputs: source_batch,\n","                #                                               dec_inputs: target_batch[:, :-1],\n","                #                                               targets: target_batch[:, 1:]})\n","\n","                # accuracy = np.mean(batch_logits.argmax(axis=-1) == target_batch[:,1:])\n","                accuracy = np.mean(train_acc)\n","                print('Epoch {:3} Loss: {:>6.3f} Accuracy: {:>6.4f} Epoch duration: {:>6.3f}s'.format(epoch_i, batch_loss,\n","                                                                                  accuracy, time.time() - start_time))\n","\n","                if epoch_i%test_steps==0:\n","                    acc_avg, acc, sensitivity, specificity, PPV= test_model()\n","\n","                    print('loss {:.4f} after {} epochs (batch_size={})'.format(loss_track[-1], epoch_i + 1, batch_size))\n","                    save_path = os.path.join(checkpoint_dir, ckpt_name)\n","                    saver.save(sess, save_path)\n","                    print(\"Model saved in path: %s\" % save_path)\n","\n","                    # if np.nan_to_num(acc_avg) > pre_acc_avg:  # save the better model based on the f1 score\n","                    #     print('loss {:.4f} after {} epochs (batch_size={})'.format(loss_track[-1], epoch_i + 1, batch_size))\n","                    #     pre_acc_avg = acc_avg\n","                    #     save_path =os.path.join(checkpoint_dir, ckpt_name)\n","                    #     saver.save(sess, save_path)\n","                    #     print(\"The best model (till now) saved in path: %s\" % save_path)\n","\n","            plt.plot(loss_track)\n","            plt.show()\n","        print(str(datetime.now()))\n","        # test_model()\n"]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"id":"FIVNE4g0m2xr","executionInfo":{"status":"error","timestamp":1640856095214,"user_tz":-480,"elapsed":524,"user":{"displayName":"溫子謙","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04405529575981230575"}},"outputId":"954f2f63-f50e-44ec-afa3-a8214e5b06b2"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]},{"output_type":"stream","name":"stderr","text":["usage: ipykernel_launcher.py [-h] [--epochs EPOCHS] [--max_time MAX_TIME]\n","                             [--test_steps TEST_STEPS]\n","                             [--batch_size BATCH_SIZE] [--data_dir DATA_DIR]\n","                             [--bidirectional BIDIRECTIONAL]\n","                             [--num_units NUM_UNITS]\n","                             [--n_oversampling N_OVERSAMPLING]\n","                             [--checkpoint_dir CHECKPOINT_DIR]\n","                             [--ckpt_name CKPT_NAME]\n","                             [--classes CLASSES [CLASSES ...]]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-d9a77795-326c-467b-910a-092f521f3401.json\n"]},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]},{"cell_type":"code","source":["!bash"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WfJ0PMoap4MA","executionInfo":{"status":"ok","timestamp":1640856497994,"user_tz":-480,"elapsed":74214,"user":{"displayName":"溫子謙","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04405529575981230575"}},"outputId":"bb68bfbe-9721-40a5-a218-b8c89f9102c5"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["bash: cannot set terminal process group (62): Inappropriate ioctl for device\n","bash: no job control in this shell\n","\u001b[1;36m/content\u001b[m# ls\n","\u001b[0m\u001b[01;34mdrive\u001b[0m  \u001b[01;34msample_data\u001b[0m\n","\u001b[1;36m/content\u001b[m# cd drive\n","\u001b[1;36m/content/drive\u001b[m# ls\n","\u001b[0m\u001b[01;34mMyDrive\u001b[0m  \u001b[01;34mOthercomputers\u001b[0m  \u001b[01;34mShareddrives\u001b[0m\n","\u001b[1;36m/content/drive\u001b[m# \n","\u001b[1;36m/content/drive\u001b[m# \n","\u001b[1;36m/content/drive\u001b[m# \n","\u001b[1;36m/content/drive\u001b[m# exit\n"]}]},{"cell_type":"code","source":["console.start()  # and click link"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MN_yNw2jlORk","executionInfo":{"status":"ok","timestamp":1640856376538,"user_tz":-480,"elapsed":347,"user":{"displayName":"溫子謙","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04405529575981230575"}},"outputId":"930596dd-f849-41ee-f84d-3c4404699c0d"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"1W8RJk0iqVAe"},"execution_count":null,"outputs":[]}]}